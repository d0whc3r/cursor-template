# Usage Examples

## Overview

This document provides practical examples of how to use Cursor Rules in real development scenarios. Each example demonstrates the complete workflow from rule activation to task completion.

## Example 1: Feature Development

### **Scenario**

Adding user authentication to a Node.js API application.

### **User Request**

```
"Add user authentication to our API with JWT tokens, password hashing, and protected routes"
```

### **AI Response with Rules Applied**

#### **Phase 0: Reconnaissance**

```
Acknowledged. Initiating Phase 0: Reconnaissance.

**Reconnaissance Digest**
- Repository: Node.js/Express API
- Current state: No authentication system
- Dependencies: Express, bcrypt, jsonwebtoken available
- Architecture: RESTful endpoints, middleware pattern
- Database: PostgreSQL with Sequelize ORM
- Testing: Jest framework with supertest
- Quality gates: ESLint, Prettier, test coverage >80%
```

#### **Phase 1: Planning**

```
**Execution Plan**
1. Implement user model with password hashing
2. Create JWT authentication middleware
3. Add login/register endpoints
4. Update existing endpoints with auth protection
5. Add comprehensive tests
6. Update documentation

**Impact Surface**
- New files: `/src/auth/`, `/src/models/User.js`
- Modified: `/src/app.js`, `/src/routes/`
- Tests: `/tests/auth/`, `/tests/integration/`
- Dependencies: bcrypt, jsonwebtoken, express-rate-limit
```

#### **Phase 2: Execution**

```
**Implementing User Model**
âœ… Created `/src/models/User.js` with password hashing
âœ… Added user validation and error handling
âœ… Implemented password comparison methods

**Creating Authentication Middleware**
âœ… Created `/src/auth/jwtMiddleware.js`
âœ… Added token validation and user extraction
âœ… Implemented rate limiting for auth endpoints

**Adding Authentication Endpoints**
âœ… Created `/src/routes/auth.js`
âœ… Implemented login/register with validation
âœ… Added password strength requirements

**Protecting Existing Routes**
âœ… Updated `/src/routes/api.js` with auth middleware
âœ… Added role-based access control
âœ… Maintained backward compatibility
```

#### **Phase 3: Verification**

```
**Running Quality Gates**
âœ… ESLint: No errors or warnings
âœ… Prettier: Code formatting consistent
âœ… Tests: All 15 tests passing (100% coverage)
âœ… Security: No vulnerabilities detected

**End-to-End Testing**
âœ… Registration endpoint working
âœ… Login endpoint returning JWT tokens
âœ… Protected routes requiring authentication
âœ… Password hashing verified
âœ… Rate limiting functional
```

#### **Phase 4: Self-Audit**

```
**Final Report**
- **Changes Applied:**
  - âœ… `created`: `/src/models/User.js`
  - âœ… `created`: `/src/auth/jwtMiddleware.js`
  - âœ… `created`: `/src/routes/auth.js`
  - âœ… `modified`: `/src/routes/api.js`
  - âœ… `created`: `/tests/auth/`
  - âœ… `modified`: `/package.json`

- **Verification Evidence:**
  - âœ… All tests passing (15/15)
  - âœ… ESLint and Prettier clean
  - âœ… API endpoints responding correctly
  - âœ… Authentication flow verified

- **Final Verdict:**
  - `Self-Audit Complete. System state verified and consistent. Mission accomplished.`
```

## Example 2: Bug Diagnosis and Fix

### **Scenario**

API randomly returning 500 errors under load.

### **User Request**

```
"The API is randomly failing with 500 errors when we have more than 100 concurrent users"
```

### **AI Response with Rules Applied**

#### **Deep Investigation Protocol**

```
âš ï¸ Initiating deep bug diagnosis protocol.

**Phase 0: Deep Reconnaissance**
- Analyzing error logs and patterns
- Investigating system dependencies
- Checking resource utilization
- Examining error correlation with system state

**Error Pattern Analysis**
âœ… Error logs show correlation with high memory usage
âœ… Database connection pool exhausted
âœ… Memory leaks in request handling
âœ… No proper error boundaries in middleware
```

#### **Root Cause Identification**

```
**Root Cause Analysis**
- **Primary Issue**: Database connection pool exhaustion
- **Secondary Issue**: Memory leaks in request processing
- **Contributing Factor**: Missing error boundaries
- **Trigger**: Concurrent user load >100

**Technical Details**
- Connection pool size: 10 (too small for load)
- Connection timeout: 30s (too long)
- No connection cleanup on errors
- Memory accumulation in request objects
```

#### **Comprehensive Fix Implementation**

```
**Implementing Fixes**
âœ… Increased connection pool size to 50
âœ… Added connection timeout to 10s
âœ… Implemented proper connection cleanup
âœ… Added error boundaries in middleware
âœ… Added memory monitoring and alerts
âœ… Implemented graceful degradation

**Testing Fixes**
âœ… Load testing with 200 concurrent users
âœ… Memory usage monitoring
âœ… Connection pool health checks
âœ… Error rate monitoring
```

#### **Verification and Documentation**

```
**Final Report**
- **Changes Applied:**
  - âœ… `modified`: `/src/database/connection.js`
  - âœ… `modified`: `/src/middleware/errorHandler.js`
  - âœ… `modified`: `/src/app.js`
  - âœ… `created`: `/src/monitoring/`

- **Verification Evidence:**
  - âœ… Load test: 200 users, 0 errors
  - âœ… Memory usage: Stable under load
  - âœ… Connection pool: Healthy utilization
  - âœ… Error rate: <0.1% under normal load

- **Final Verdict:**
  - `Self-Audit Complete. System state verified and consistent. Mission accomplished.`
```

## Example 3: Code Refactoring

### **Scenario**

Refactoring a monolithic service into microservices.

### **User Request**

```
"Refactor our monolithic user service into microservices for better scalability"
```

### **AI Response with Rules Applied**

#### **System Analysis**

```
Acknowledged. Initiating Phase 0: Reconnaissance.

**Reconnaissance Digest**
- Current: Monolithic Node.js service (15K lines)
- Architecture: Single database, shared state
- Dependencies: 45 npm packages
- Testing: 80% coverage, integration tests
- Deployment: Single container, manual deployment
- Monitoring: Basic logging, no metrics
```

#### **Strategic Planning**

```
**Refactoring Strategy**
1. **Service Decomposition**: Identify bounded contexts
2. **Database Separation**: Split data by domain
3. **API Gateway**: Implement service discovery
4. **Event System**: Replace direct calls with events
5. **Independent Deployment**: Containerize each service

**Service Boundaries Identified**
- User Management Service
- Authentication Service
- Profile Service
- Notification Service
- Analytics Service
```

#### **Incremental Implementation**

```
**Phase 1: Service Extraction**
âœ… Created User Management Service
âœ… Implemented service-to-service communication
âœ… Added health checks and monitoring
âœ… Maintained backward compatibility

**Phase 2: Database Separation**
âœ… Split user data by domain
âœ… Implemented data migration scripts
âœ… Added data consistency checks
âœ… Updated integration tests

**Phase 3: API Gateway**
âœ… Implemented service discovery
âœ… Added load balancing
âœ… Implemented circuit breakers
âœ… Added request routing
```

#### **Quality Assurance**

```
**Comprehensive Testing**
âœ… Unit tests: All services >90% coverage
âœ… Integration tests: Service communication verified
âœ… Load tests: Performance maintained
âœ… Security tests: No vulnerabilities introduced

**Deployment Verification**
âœ… All services deploying independently
âœ… Health checks passing
âœ… Monitoring dashboards functional
âœ… Rollback procedures tested
```

#### **Final Validation**

```
**Final Report**
- **Changes Applied:**
  - âœ… `created`: `/services/user-management/`
  - âœ… `created`: `/services/authentication/`
  - âœ… `created`: `/services/profile/`
  - âœ… `created`: `/services/notification/`
  - âœ… `created`: `/services/analytics/`
  - âœ… `created`: `/gateway/`
  - âœ… `modified`: `/deployment/`

- **Verification Evidence:**
  - âœ… All services operational
  - âœ… Performance: 2x improvement
  - âœ… Deployment time: 80% reduction
  - âœ… Test coverage: 92% overall

- **Final Verdict:**
  - `Self-Audit Complete. System state verified and consistent. Mission accomplished.`
```

## Example 4: Documentation Generation

### **Scenario**

Creating technical architecture documentation.

### **User Request**

```
"Create a technical architecture document for our new microservices system"
```

### **AI Response with Rules Applied**

#### **Document Generation**

```
ğŸ”„ Generating technical architecture document using steering rules.

**Technical Architecture Document**
*Author: Lead Architect*
*Date: 2024-01-15*
*Version: 1.0*

**Executive Summary**
Document the microservices architecture for improved scalability and maintainability.

**Current Architecture**
- Monolithic Node.js application
- Single PostgreSQL database
- Manual deployment process
- Limited monitoring and observability

**Technical Requirements**
- Support 10x user growth
- Enable independent service deployment
- Maintain 99.9% uptime
- Support multiple development teams

**Architecture Options Considered**
- Service mesh with Istio
- API gateway with Kong
- Event-driven architecture with Kafka
- Database per service pattern

**Recommended Architecture**
- Service mesh using Istio for communication
- API gateway for external access
- Event-driven communication with Kafka
- Database per service with eventual consistency
- Kubernetes orchestration for deployment

**Implementation Plan**
- Phase 1: Infrastructure and service mesh setup
- Phase 2: Core service extraction
- Phase 3: Advanced features and optimization
- Phase 4: Performance tuning and monitoring

**Risk Mitigation**
- Gradual migration to minimize disruption
- Comprehensive testing and rollback procedures
- Performance monitoring and alerting
- Team training and documentation

**Success Criteria**
- 50% reduction in deployment time
- 99.9% system uptime maintained
- 3x increase in development velocity
- Successful migration of all core services
```

## Example 5: Team Retrospective

### **Scenario**

End-of-sprint retrospective for development team.

### **User Request**

```
"Let's do a retro on this sprint's work"
```

### **AI Response with Rules Applied**

#### **Retrospective Analysis**

```
ğŸ”„ Initiating retrospective analysis.

**Sprint Review**
- **Duration**: 2 weeks
- **Team**: 6 developers, 1 PM, 1 QA
- **Objectives**: User authentication system, API improvements
- **Completion**: 85% of planned work

**Work Completed**
âœ… User authentication with JWT
âœ… API rate limiting and security
âœ… Database connection optimization
âœ… Comprehensive test coverage
âš ï¸ Mobile app UI incomplete (carried over)

**Quality Metrics**
- Code coverage: 92% (target: 90%)
- Bug count: 3 (target: <5)
- Deployment success: 100% (target: >95%)
- Code review time: 2.4 hours (target: <4 hours)
```

#### **Learning Extraction**

```
**Key Learnings**
- **Positive**: Test-driven development improved code quality
- **Positive**: Pair programming reduced bugs by 40%
- **Challenge**: Database migrations need better tooling
- **Challenge**: Mobile development underestimated complexity

**Process Improvements Identified**
- Implement automated database migration testing
- Add mobile development expertise to team
- Standardize code review checklists
- Improve sprint planning accuracy
```

#### **Action Planning**

```
**Next Sprint Actions**
- **Database Team**: Research migration testing tools
- **Mobile Team**: Complete UI implementation
- **QA Team**: Implement automated testing pipeline
- **PM**: Refine sprint planning process

**Process Updates**
- Add database migration testing to definition of done
- Include mobile development complexity in story points
- Implement code review templates
- Add sprint retrospective to regular cadence
```

## Best Practices Summary

### **Rule Selection**

1. **Assess Task Type**: Choose appropriate protocol
2. **Consider Complexity**: Simple vs. complex requirements
3. **Evaluate Context**: Current system state and constraints
4. **Match Protocol**: Select workflow that fits the need

### **Execution Quality**

1. **Follow All Phases**: Don't skip verification steps
2. **Use Status Markers**: Clear progress indication
3. **Maintain Documentation**: Keep chat logs for reference
4. **Complete Self-Audit**: Always verify final state

### **Continuous Improvement**

1. **Regular Retros**: End sessions with improvement analysis
2. **Rule Evolution**: Update protocols based on learnings
3. **Pattern Recognition**: Identify effective approaches
4. **Knowledge Sharing**: Document successful strategies

---

**Note**: These examples demonstrate the core rules from the [Autonomous Agent Prompting Framework](https://gist.github.com/aashari/07cc9c1b6c0debbeb4f4d94a3a81339e) by aashari. For detailed information about the rules, see the [Core Rules Documentation](./rules/).
